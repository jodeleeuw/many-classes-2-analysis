---
title: "ManyClasses 2 Analysis"
format: html
editor: visual
---

```{r}
#| label: Load packages
#| include: false
library(osfr)
library(brms)
library(tidyr)
library(dplyr)
library(tidybayes)
library(ggplot2)
library(ggdist)
library(bayesplot)
library(forcats)
library(stringr)
library(gt)
```

```{r}
#| label: Configuration
#| include: false

options(dplyr.summarise.inform = FALSE)
```

```{r}
#| label: Get data from OSF
#| include: false

osf_retrieve_node("https://osf.io/t5ma3/") %>%
  osf_ls_files(path="Analysis", pattern=".Rdata") %>%
  osf_download(path="data", conflicts="skip")
```

```{r}
#| label: Load data
#| include: false

load("data/processed_data.Rdata")
load("data/analysis_set.Rdata")

```

# Descriptives

```{r}
#| label: Descriptives
#| include: false

# How many classes?
descriptives_num_classes <- length(unique(classes$course_id))

# How many institutions/campuses?
descriptives_num_institutions <- length(unique(classes$affiliation))

# How many enrolled students?
descriptives_enrolled_student_sum <- sum(experiment$enrollment_cnt)
descriptives_enrolled_student_per_class_mean <- mean(experiment$enrollment_cnt)

# How many consenting participants?
descriptives_consenting_participant_sum <- length(unique(participants$participant_id))
descriptives_consenting_participant_per_class_mean <- participants %>% 
  group_by(course_id) %>% 
  summarize(n=length(unique(participant_id))) %>% 
  summarize(mean=mean(n)) %>% 
  pull()
descriptives_consenting_participant_per_class_sd <- participants %>% 
  group_by(course_id) %>% 
  summarize(n=length(unique(participant_id))) %>% 
  summarize(sd=sd(n)) %>% 
  pull()

# How many consenting participants with outcome scores?
descriptives_consenting_with_outcomes <- datFrm %>% summarize(sum=length(unique(participant_id)))
```

Summary of participants:

- There are `r descriptives_num_classes` classes across `r descriptives_num_institutions` institutions.
- There are `r descriptives_enrolled_student_sum` enrolled students, with an average of `r descriptives_enrolled_student_per_class_mean` students per class.
- There are `r descriptives_consenting_participant_sum` consenting participants, with an average of `r descriptives_consenting_participant_per_class_mean` participants per class (SD = `r descriptives_consenting_participant_per_class_sd`).
- There are `r descriptives_consenting_with_outcomes$sum` consenting participants with outcome scores.

```{r}
#| label: tbl-course-descriptives
#| echo: false
#| tbl-cap: Summary of each course

course_descriptives <- tibble(
  course_id = classes %>% pull(course_id),
  level = outcomes %>% 
    filter(outcome_name == "Level") %>% 
    select(course_id, outcome_score) %>% 
    mutate(outcome_score = if_else(outcome_score < 6, outcome_score + 12, outcome_score)) %>% 
    mutate(outcome_score = case_when(outcome_score <= 9 ~ "Middle School", outcome_score <= 12 ~ "High School", TRUE ~ "College")) %>%
    group_by(course_id) %>%
    slice(1) %>%
    pull(outcome_score),
  enrollment = experiment %>%
    group_by(course_id) %>%
    summarize(enrollment = sum(enrollment_cnt)) %>%
    pull(enrollment),
  participants = datFrm %>% 
    group_by(course_id) %>% 
    summarize(participants = length(unique(participant_id))) %>% 
    pull(participants),
  prequestion_accuracy = datFrm %>% 
    group_by(course_id) %>% 
    summarize(prequestion_accuracy = mean(prequestion_score)) %>% 
    pull(prequestion_accuracy)
) %>% 
  mutate(participation_rate = participants / enrollment)

gt(course_descriptives) %>%
  grand_summary_rows(columns = c("enrollment", "participants", "prequestion_accuracy", "participation_rate"), fns = "mean")

```


```{r}
#| label: tbl-institutions
#| echo: false
#| tbl-cap: Number of classes per institution

classes %>% group_by(affiliation) %>% summarize(n=length(unique(course_id))) %>% gt()
```

## Treatment characteristics

```{r}
#| label: Treatment characteristics
#| include: false

treatment_pct_watched_both <- datFrm %>% 
  pivot_longer(
    cols = c(submit_flag_prequestions,submit_flag_control), 
    names_to = "condition", 
    values_to="submission_flag"
  ) %>% 
  summarize(pctSubmittingBoth = mean(submission_flag))

treatment_pct_watched_by_condiiton <- datFrm %>% 
  summarize(pct_submit_prequesitons = mean(submit_flag_prequestions),
            pct_submit_control = mean(submit_flag_control))

treatment_median_time_spent_on_assignment <- datFrm %>% 
  pivot_longer(
    cols = c(duration_prequestions,duration_control),
    names_to = "condition",
    values_to="duration"
  ) %>% 
  select(participant_id,condition,duration) %>% 
  na.omit() %>% 
  summarize(median_duration = median(duration)/60) %>%
  pull()

treatment_median_time_spent_on_assignment_by_condition <- datFrm %>% 
  pivot_longer(
    cols = c(duration_prequestions,duration_control),
    names_to = "condition",
    values_to="duration"
  ) %>% 
  select(participant_id,condition,duration) %>% 
  na.omit() %>% 
  group_by(condition) %>% 
  summarize(median_duration = median(duration)/60)

treatment_class_average_min_and_max_score_on_prequestions <- datFrm %>% 
  group_by(course_id) %>% 
  summarize(class_prequestion = mean(prequestion_score)) %>% 
  ungroup() %>% 
  summarize(
    mean_prequestion = mean(class_prequestion),
    min_prequestion = min(class_prequestion),
    max_prequestion = max(class_prequestion))
```

Treatment characteristics summary:

- The average proportion of students who watched both prequestions and control assignments is `r treatment_pct_watched_both$pctSubmittingBoth`.
- The average proportion of students who watched the prequestions assignment is `r treatment_pct_watched_by_condiiton$pct_submit_prequesitons` and the control assignment is `r treatment_pct_watched_by_condiiton$pct_submit_control`.
- The median time spent on the assignment was `r treatment_median_time_spent_on_assignment` minutes.
- The median time spent on the assignment by condition was `r treatment_median_time_spent_on_assignment_by_condition %>% filter(condition=="prequestions") %>% pull(median_duration) ` for prequestions and `r treatment_median_time_spent_on_assignment_by_condition %>% filter(condition=="control") %>% pull(median_duration)` for
- The average prequestion score across classes was `r treatment_class_average_min_and_max_score_on_prequestions$mean_prequestion`, with a minimum of `r treatment_class_average_min_and_max_score_on_prequestions$min_prequestion` and a maximum of `r treatment_class_average_min_and_max_score_on_prequestions$max_prequestion`.

## Effect of Prequestions on Student Behavior

```{r}
#| label: Effect of Prequestions on Student Behavior
#| include: false

# Percent of students who watched videos in their entirety
student_beh_pct_watched_entire_video <- datFrm %>% 
  pivot_longer(cols = c(viewpct_prequestions,viewpct_control),names_to = "condition",values_to="view_pct") %>% 
  mutate(watchwhole = if_else(view_pct == 1,1,0)) %>% 
  summarize(pct_watchwhole = mean(watchwhole))

# Average percent of videos viewed
student_beh_avg_pct_viewed <- datFrm %>% 
  pivot_longer(cols = c(viewpct_prequestions,viewpct_control),names_to = "condition",values_to="view_pct") %>%
  summarize(mean_viewpct = mean(view_pct))

# Average percent of videos viewed by condition
student_beh_avg_pct_viewed_by_condition <- datFrm %>% 
  summarize(viewpct_prequestions = mean(viewpct_prequestions),
                     viewpct_control = mean(viewpct_control))

# Likelihood of not initiating media playback
student_beh_proportion_who_didnt_initiate_video_playback <- datFrm %>% 
  mutate(doNotWatch_preq = if_else(viewpct_prequestions == 0.00,1,0),
                  doNotWatch_cont = if_else(viewpct_control == 0.00,1,0)) %>% 
  summarize(doNotWatch_preq = mean(doNotWatch_preq), 
            doNotWatch_cont = mean(doNotWatch_cont))

# Stability across classes
student_beh_didnt_watch_by_class <- datFrm %>% 
  mutate(doNotWatch_preq = if_else(viewpct_prequestions == 0.00,1,0),
                  doNotWatch_cont = if_else(viewpct_control == 0.00,1,0)) %>% 
  group_by(course_id) %>% 
  summarize(doNotWatch_preq = mean(doNotWatch_preq)*100,
            doNotWatch_cont = mean(doNotWatch_cont)*100) %>%
  mutate(doNotWatch_diff = (doNotWatch_preq-doNotWatch_cont))

# If you filter out the students who did watch with control and did not watch with
# prequestions, is there a difference in view_pct across the remaining student
# sample?
datFrm_students_who_watched_both_videos <- datFrm %>% 
  filter(viewpct_prequestions > 0.00 & viewpct_control > 0.00)

student_beh_pct_who_watched_both_videos <- nrow(datFrm_students_who_watched_both_videos)/nrow(datFrm)
  
student_beh_watched_both_by_class <- datFrm_students_who_watched_both_videos %>%
  group_by(course_id) %>% 
  summarize(viewpct_prequestions = mean(viewpct_prequestions),
            viewpct_control = mean(viewpct_control),
            nevents_prequestions = mean(nevents_prequestions),
            nevents_control = mean(nevents_control)) %>%
  ungroup() %>% 
  summarize(viewpct_prequestions = mean(viewpct_prequestions),
            viewpct_control = mean(viewpct_control),
            nevents_prequestions = mean(nevents_prequestions),
            nevents_control = mean(nevents_control))
```

- The average proportion of students who watched the entire video is `r student_beh_pct_watched_entire_video$pct_watchwhole`.
- The average percentage of the video viewed is `r student_beh_avg_pct_viewed$mean_viewpct`.
- The average percentage of the video viewed by condition is `r student_beh_avg_pct_viewed_by_condition$viewpct_prequestions` for prequestions and `r student_beh_avg_pct_viewed_by_condition$viewpct_control` for control.
- The likelihood of not initiating media playback is `r student_beh_proportion_who_didnt_initiate_video_playback$doNotWatch_preq` for prequestions and `r student_beh_proportion_who_didnt_initiate_video_playback$doNotWatch_cont` for control.
- The proportion of students who watched both videos is `r student_beh_pct_who_watched_both_videos`.
- The average percentage of the video viewed by class for students who watched both videos is `r student_beh_watched_both_by_class$viewpct_prequestions` for prequestions and `r student_beh_watched_both_by_class$viewpct_control` for control.

```{r}
#| label: tbl-difference-in-playback-by-class
#| echo: false
#| tbl-cap: Difference in the percentage of students who did not initiate media playback by class, by condition

student_beh_didnt_watch_by_class %>% arrange(desc(doNotWatch_diff)) %>% gt() %>% fmt_number()
```

```{r}
#| label: fig-something
#| echo: false
#| fig-cap: Difference in playback percentage. Thick line is average. Thin lines are individual classes, with opacity indicating number of students.

# Make a figure
load(file = "data/view_df.Rdata")

view_df <- merge(view_df,participants,by="participant_id",all.x=TRUE) %>% 
  select(!c(consented_at, consent_source))

view_summ <- view_df %>% 
  group_by(condition_name, percent, course_id) %>% 
  summarize(view_pct = mean(view), n = length(unique(participant_id))) %>% 
  pivot_wider(id_cols = c(percent,course_id,n), names_from = condition_name,values_from = view_pct) %>% 
  mutate(diff = `Pre-Questions` - Control) %>% 
  select(percent,course_id,diff,n) %>% 
  na.omit()

view_summ2 <- view_summ %>% 
  group_by(percent) %>% 
  summarize(diff = mean(diff))

ggplot(data = view_summ, aes(x=percent,y=diff)) +
  geom_line(aes(group = course_id,alpha=n)) +
  geom_line(data=view_summ2,aes(x=percent,y=diff), linewidth=3) +
  geom_hline(yintercept=0) +
  scale_x_continuous(limits = c(0,100), expand = c(0,0), labels=scales::percent_format(scale=1)) +
  scale_y_continuous(labels=scales::percent_format(scale=100)) +
  xlab("Video Playback") +
  ylab("Percent Difference\n(Prequestion - Control)") +
  theme(panel.grid=element_blank(), panel.background = element_blank(),legend.position="none",
        axis.line.x = element_line(color="black"),
        axis.line.y = element_line(color="black"))
```


## Effect of Prequestions on Student Learning

### Overall outcome accuracy

```{r}
#| label: Overall outcome accuracy
#| echo: false
outcome_accuracy <- datFrm %>% 
  mutate(outcomePreq = outcome_prequestions/points_possible_prequestions,
         outcomeCont = outcome_control/points_possible_control) %>% 
  pivot_longer(cols = c(outcomePreq,outcomeCont), names_to = "condition",values_to = "outcome") %>% 
  group_by(course_id) %>%
  summarize(course_level_outcome = mean(outcome)) %>%
  summarize(mean_outcome = mean(course_level_outcome),
            sd_outcome = sd(course_level_outcome),
            se_outcome = sd_outcome/sqrt(n()))
```

The overall outcome accuracy is `r outcome_accuracy$mean_outcome` (SD = `r outcome_accuracy$sd_outcome`, SE = `r outcome_accuracy$se_outcome`).

### Split by condition

```{r}
#| label: Outcome split by condition
#| echo: false

outcome_by_condition <- datFrm %>% 
  mutate(outcomePreq = outcome_prequestions/points_possible_prequestions,
         outcomeCont = outcome_control/points_possible_control) %>% 
  group_by(course_id) %>% 
  summarize(class_preqLearn = mean(outcomePreq),
            class_contLearn = mean(outcomeCont)) %>% 
  summarize(preqLearn = mean(class_preqLearn), preqLearnSD = sd(class_preqLearn), preqLearnSE = preqLearnSD/sqrt(n()),
            contLearn = mean(class_contLearn), contLearnSD = sd(class_contLearn), contLearnSE = contLearnSD/sqrt(n()))
```

In the control condition, the average outcome accuracy was `r outcome_by_condition$contLearn` (SD = `r outcome_by_condition$contLearnSD`, SE = `r outcome_by_condition$contLearnSE`), and in the prequestions condition, the average outcome accuracy was `r outcome_by_condition$preqLearn` (SD = `r outcome_by_condition$preqLearnSD`, SE = `r outcome_by_condition$preqLearnSE`).

### What percent of classes had higher performance in the prequestion condition?

```{r}
#| label: Prequestion performance vs control performance by class
#| include: false

performance_better_in_preq_counts <- datFrm %>% 
  mutate(outcomePreq = outcome_prequestions/points_possible_prequestions,
         outcomeCont = outcome_control/points_possible_control) %>% 
  group_by(course_id) %>% 
  summarize(class_preqLearn = mean(outcomePreq),
            class_contLearn = mean(outcomeCont)) %>%
  mutate(preqBetter = if_else(class_preqLearn > class_contLearn,1,0)) %>% 
  summarize(count = sum(preqBetter), pct = mean(preqBetter))




```

Of the 30 classes, `r performance_better_in_preq_counts$count` (`r performance_better_in_preq_counts$pct*100`%) had higher performance in the prequestions condition.

```{r}
#| label: tbl-prequestion-performance
#| echo: false

datFrm %>% 
  mutate(outcomePreq = outcome_prequestions/points_possible_prequestions,
         outcomeCont = outcome_control/points_possible_control) %>% 
  group_by(course_id) %>% 
  summarize(class_preqLearn = mean(outcomePreq), 
            class_contLearn = mean(outcomeCont)) %>%
  mutate(preqBetter = class_preqLearn - class_contLearn) %>% 
  arrange(desc(preqBetter)) %>%
  gt()
```


```{r}
#| label: Prepare data
#| include: false

overall_model_data <- datFrm %>%
  select(participant_id, course_id, outcome_prequestions, 
         points_possible_prequestions, outcome_control, 
         points_possible_control, exposure_order) %>%
  pivot_longer(cols = c(outcome_prequestions, outcome_control), 
               names_to = "outcome_phase", values_to = "score",
               names_prefix = "outcome_") %>%
  pivot_longer(cols = c(points_possible_prequestions, points_possible_control), 
    names_to = "trials_phase", 
    values_to = "trials", 
    names_prefix = "points_possible_") %>%
  filter(outcome_phase == trials_phase) %>%
  mutate(condition = outcome_phase) %>%
  select(-trials_phase, -outcome_phase)

```

```{r}
#| label: fig-raw-data
#| echo: false
#| fig-cap: The benefit of prequestions for each class, calculated as the proportion of correct responses in the prequestions condition minus the proportion of correct responses in the control condition. Dots are the mean and lines show +/- 1SE.

raw_data_summary <- overall_model_data %>%
  mutate(proportion_correct = score / trials) %>%
  pivot_wider(id_cols = c(participant_id, course_id), names_from=condition, values_from = proportion_correct) %>%
  mutate(prequestion_benefit = prequestions - control) %>%
  group_by(course_id) %>%
  mutate(course_id = factor(course_id)) %>%
  summarize(M = mean(prequestion_benefit), SD = sd(prequestion_benefit), SE = sd(prequestion_benefit) / sqrt(n()))

raw_data_summary$course_id <- fct_reorder(raw_data_summary$course_id, raw_data_summary$M, median)


ggplot(raw_data_summary, aes(x=M, y=course_id, xmin=M-SE, xmax=M+SE))+
  geom_pointrange()+
  theme_minimal()+
  theme(panel.grid.major.y=element_blank())+
  labs(x="Proportion correct with prequestions - proportion correct without",
       y="Class ID")
```

# Overall Model

```{r}
#| label: Helper function for running models
#| include: false

run_brms_model <- function(filename, formula, data, priors, bulk_ess_threshold=1000, family=binomial(link = "logit")) {
  
  cores_to_use <- 6
  
  filepath <- paste0("fits/", filename, ".rds")
  
  if(file.exists(filepath)){
    fit <- readRDS(filepath)
  } else {
    fit <- brm(
      formula = formula,
      data = data,
      family = family,
      chains = cores_to_use,
      iter = bulk_ess_threshold + 1000,
      warmup = 1000,
      cores = cores_to_use,
      control=list(
        adapt_delta=0.99
      ),
      prior=priors
    )
    
    ms <- summary(fit)
      
    ess <- c(
      sapply(ms$random, function(x){ return(x$Bulk_ESS)}, simplify = TRUE) %>% unlist(),
      ms$fixed$Bulk_ESS
    )
    
    min_ess <- min(ess)
    
    while(min_ess < bulk_ess_threshold){
      new_fit <- brm(
        formula = formula,
        data = data,
        family = family,
        chains = cores_to_use,
        iter = bulk_ess_threshold + 1000,
        warmup = 1000,
        cores = cores_to_use,
        control=list(
          adapt_delta=0.99
        ),
        prior=priors
      )
      fit <- combine_models(fit, new_fit)
      
      ms <- summary(fit)
      
      ess <- c(
        sapply(ms$random, function(x){ return(x$Bulk_ESS)}, simplify = TRUE) %>% unlist(),
        ms$fixed$Bulk_ESS
      )
      
      min_ess <- min(ess)

    }
    saveRDS(fit, filepath)
  }
  return(fit)
}
```

This model is `score | trials(trials) ~ condition + (0 + condition + exposure_order | course_id) + (1 | course_id/participant_id)`. A fixed effect of condition (prequestions vs. control), random effects of condition and exposure per class, and random intercept of participant , nested in random intercept of class. Exposure is included as a random effect because there are two assignments per class, and each assignment may vary in difficulty or other factors that could affect the outcome.

```{r}
#| label: Define priors
#| include: false

priors <- c(
  set_prior("normal(0, 0.5)", class="b"),
  set_prior("gamma(1.64, 0.32)", class="sd")
)

```

```{r}
#| label: Overall model
#| include: false

overall_fit <- run_brms_model(
  filename="overall", 
  formula=bf(score | trials(trials) ~ condition + (0 + condition + exposure_order | course_id) + (1 | course_id/participant_id)),
  data=overall_model_data,
  priors=priors
)
```

```{r}
#| label: fig-overall-forest-plot
#| echo: false
#| fig-cap: Model estimates of the benefit of prequestions for each class, shown as an odds ratio (probability of correct responses in prequestions / probability of correct responses in control). The circle is the median of the posterior, the thicker line is a 66% highest-density interval, and the thin line is a 95% highest-density interval. The diamond at the bottom shows the overall effect across classes.
#| fig-width: 10

forest_plot_data <- overall_fit %>%
  spread_draws(b_Intercept, b_conditionprequestions, r_course_id[id, condition]) %>%
  filter(condition %in% c("conditionprequestions", "Intercept")) %>%
  pivot_wider(names_from = condition, values_from = r_course_id) %>%
  mutate(id = factor(id), 
         course_effect = conditionprequestions + b_conditionprequestions,
         odds = exp(course_effect)
  )
forest_plot_data$id <- fct_reorder(forest_plot_data$id, forest_plot_data$odds, median)

overall_estimate_forest_plot <- forest_plot_data %>%
  filter(id==98) %>%
  mutate(odds = exp(b_conditionprequestions),
         id=0)
  
ggplot(forest_plot_data, aes(x = odds, y=id)) +
  stat_pointinterval()+
  stat_pointinterval(data=overall_estimate_forest_plot, shape=18, point_size=5)+
  labs(x="Odds ratio for correct response with prequestions / without prequestions ", y="Class ID")+
  scale_x_continuous(breaks=c(seq(0,1,0.1),seq(1,10,1)))+
  coord_trans(x = 'log10') +
  geom_vline(xintercept=1)+
  theme_minimal()+
  theme(panel.grid.major.y = element_blank(), panel.grid.minor.x = element_blank())
  
```

# Moderators

```{r}
#| label: Load moderators
#| include: false

load('moderators.Rdata')
```

## Class Level

### Exposure order

We confirm here that there's no consistent fixed effect of exposure order.

```{r}
#| label: Exposure order data
#| include: false

overall_model_data <- datFrm %>%
  select(participant_id, course_id, outcome_prequestions, 
         points_possible_prequestions, outcome_control, 
         points_possible_control, exposure_order) %>%
  pivot_longer(cols = c(outcome_prequestions, outcome_control), 
               names_to = "outcome_phase", values_to = "score",
               names_prefix = "outcome_") %>%
  pivot_longer(cols = c(points_possible_prequestions, points_possible_control), 
    names_to = "trials_phase", 
    values_to = "trials", 
    names_prefix = "points_possible_") %>%
  filter(outcome_phase == trials_phase) %>%
  mutate(condition = outcome_phase) %>%
  select(-trials_phase, -outcome_phase)

```

```{r}
#| label: Exposure fixed effect
#| include: false

class_level_exposure_fit <- run_brms_model(
  "class_level_exposure",
  bf(score | trials(trials) ~ condition*exposure_order + (0 + condition + exposure_order | course_id) + (1 | course_id/participant_id)),
  overall_model_data,
  priors
)
```

```{r}
#| echo: false
fixef(class_level_exposure_fit)
```

### Number of Assignments

```{r}
#| label: Merge class level moderators
#| include: false

mod_class_scaled <- mod_class %>%
  mutate(nbr_other_assessments = nbr_other_assessments / max(nbr_other_assessments))

class_level_model_data <- overall_model_data %>%
  left_join(mod_class_scaled, by=join_by(course_id))
```

```{r}
#| label: Class-level number of assignments
#| include: false

class_level_number_assignments_fit <- run_brms_model(
  "class_level_number_assignments",
  bf(score | trials(trials) ~ condition*nbr_other_assessments + (1 | course_id/participant_id) + (0 + condition + exposure_order | course_id)),
  class_level_model_data,
  priors
)
```

```{r}
#| echo: false
fixef(class_level_number_assignments_fit)
```

## Student Level

```{r}
#| label: Create student-level model data
#| include: false

mod_student_scaled <- mod_student %>%
  mutate(pretest = pretest / max(pretest, na.rm = TRUE),
         prequestion_score = prequestion_score / max(prequestion_score))

student_level_model_data <- overall_model_data %>%
  left_join(mod_student_scaled, by=join_by(participant_id, course_id)) %>%
  mutate(level = case_when(
    level %in% c(6,7,8) ~ "middle school",
    level %in% c(9,10,11,12) ~ "high school",
    TRUE ~ "college"
  ))
```

### Grade Level

We reconceptualized this as a class-level moderator, with middle/high/college split.

```{r}
#| label: class-level grade level moderator
#| include: false

class_level_grade_level_fit <- run_brms_model(
  "class_level_grade_level",
  bf(score | trials(trials) ~ condition*level + (1 | course_id/participant_id) + (0 + exposure_order + condition | course_id)),
  student_level_model_data,
  priors
)
```

```{r}
#| echo: false
fixef(class_level_grade_level_fit)
```

### Pretest

```{r}
#| label: student-level pretest moderator
#| include: false

student_level_pretest_fit <- run_brms_model(
  "student_level_pretest",
  bf(score | trials(trials) ~ condition*pretest + (1 | course_id/participant_id) + (0 + exposure_order + condition*pretest | course_id)),
  student_level_model_data,
  priors
)
```

```{r}
#| echo: false
fixef(student_level_pretest_fit)
```

### In Major

```{r}
#| label: student-level in major moderator
#| include: false

student_level_major_fit <- run_brms_model(
  "student_level_major",
  bf(score | trials(trials) ~ condition*in_major + (1 | course_id/participant_id) + (0 + exposure_order + in_major*condition | course_id)),
  student_level_model_data,
  priors
)
```

```{r}
#| echo: false
fixef(student_level_major_fit)
```

### Prequestion Score

```{r}
#| label: student-level prequestion score moderator
#| include: false

student_level_prequestion_score_fit <- run_brms_model(
  "student_level_prequestion_score",
  bf(score | trials(trials) ~ condition*prequestion_score + (1 | course_id/participant_id) + (0 + exposure_order + prequestion_score*condition | course_id)),
  student_level_model_data,
  priors
)

```

```{r}
#| echo: false
fixef(student_level_prequestion_score_fit)
```

#### Visualization of the interaction between prequestion score and condition

```{r}
#| label: fig-prequestion-score
#| echo: false

condition_prequestion_viz_data <- student_level_model_data %>%
  mutate(assessment_accuracy = score / trials) %>%
  pivot_wider(names_from = condition, values_from = assessment_accuracy, id_cols = c(participant_id, course_id, prequestion_score)) %>%
  mutate(improvement = prequestions - control) %>%
  group_by(improvement, prequestion_score) %>%
  summarize(n=n(), .groups="drop")

ggplot(condition_prequestion_viz_data, aes(x=prequestion_score, y=improvement, size=n))+
  geom_point()+
  theme_bw()+
  labs(x="\nProportion of prequestions answered correctly", y="Improvement in final assessment with prequestions\n")+
  theme(panel.grid = element_blank())
```

## Submission Level

```{r}
#| label: Create submission-level data
#| include: false

mod_submission_scaled <- mod_submission %>%
  mutate(viewpct = viewpct / max(viewpct, na.rm = TRUE),
         nevents = nevents / max(nevents, na.rm = TRUE),
         submDaysBeforeDueDate = submDaysBeforeDueDate / max(submDaysBeforeDueDate, na.rm = TRUE))

submission_level_model_data <- overall_model_data %>%
  left_join(mod_submission_scaled, by=join_by(participant_id, course_id, condition))

```

### Percentage of video viewed

```{r}
#| label: submission-level percentage of video video
#| include: false

submission_level_percent_viewed_fit <- run_brms_model(
  "submission_level_percent_viewed",
  bf(score | trials(trials) ~ condition*viewpct + (1 | course_id/participant_id) + (0 + exposure_order + viewpct*condition | course_id)),
  submission_level_model_data,
  priors
)

```

```{r}
#| echo: false
fixef(submission_level_percent_viewed_fit)
```

### Did the student start the video at all?

```{r}
#| label: submission-level initiate playback
#| include: false

submission_level_initiate_playback_fit <- run_brms_model(
  "submission_level_initiate_playback",
  bf(score | trials(trials) ~ condition*initiate_playback + (1 | course_id/participant_id) + (0 + exposure_order + initiate_playback*condition | course_id)),
  submission_level_model_data,
  priors
)

```

```{r}
#| echo: false
fixef(submission_level_initiate_playback_fit)
```

### N Events (clicks?)

```{r}
#| label: submission-level number of events
#| include: false

submission_level_n_events_fit <- run_brms_model(
  "submission_level_n_events",
  bf(score | trials(trials) ~ condition*nevents + (1 | course_id/participant_id) + (0 + exposure_order + nevents*condition | course_id)),
  submission_level_model_data,
  priors
)

```

```{r}
#| echo: false
fixef(submission_level_n_events_fit)
```

### Submission time

```{r}
#| label: submission-level time
#| include: false

submission_level_time_fit <- run_brms_model(
  "submission_level_time",
  bf(score | trials(trials) ~ condition*submDaysBeforeDueDate + (1 | course_id/participant_id) + (0 + exposure_order + submDaysBeforeDueDate*condition | course_id)),
  submission_level_model_data,
  priors
)

```

```{r}
#| echo: false
fixef(submission_level_time_fit)
```

## Exposure Level

```{r}
#| label: Create exposure-level data
#| include: false

# scale continuous variables in mod_exposure
mod_exposure_scaled <- mod_exposure %>%
  mutate(delay = delay / max(delay, na.rm = TRUE),
         pct_correct = pct_correct / max(pct_correct, na.rm = TRUE),
         video_length = video_length / max(video_length, na.rm = TRUE),
         avg_time_of_prequestions = avg_time_of_prequestions / max(avg_time_of_prequestions, na.rm = TRUE),
         time_answering_preqs = time_answering_preqs / max(time_answering_preqs, na.rm = TRUE),
         preq_difficulty = preq_difficulty / max(preq_difficulty, na.rm = TRUE))

exposure_level_model_data <- overall_model_data %>%
  mutate(exposure = case_when(
    exposure_order == 'control_then_prequestion' & condition == 'control' ~ 1,
    exposure_order == 'control_then_prequestion' & condition == 'prequestions' ~ 2,
    exposure_order == 'prequestions_then_control' & condition == 'control' ~ 2,
    exposure_order == 'prequestions_then_control' & condition == 'prequestions' ~ 1,
  )) %>%
  left_join(mod_exposure_scaled, by=join_by(course_id, exposure)) 
```

### delay: days between assignment due date and exam date

```{r}
#| label: exposure-level delay
#| include: false

exposure_level_delay_fit <- run_brms_model(
  "exposure_level_delay",
  bf(score | trials(trials) ~ condition*delay + (1 | course_id/participant_id) + (0 + exposure + condition | course_id)),
  exposure_level_model_data,
  priors
)
```

```{r}
#| echo: false
fixef(exposure_level_delay_fit)
```

### pct_correct: overall percent correct on the exam items

```{r}
#| label: exposure-level percent correct
#| include: false

exposure_level_pct_correct_fit <- run_brms_model(
  "exposure_level_pct_correct",
  bf(score | trials(trials) ~ condition*pct_correct + (1 | course_id/participant_id) + (0 + exposure + condition | course_id)),
  exposure_level_model_data,
  priors
)

```

```{r}
#| label: exposure-level pct_correct fixed effects
#| echo: false

fixef(exposure_level_pct_correct_fit)
```

### video_length: total duration of video

```{r}
#| label: exposure-level video length
#| include: false

exposure_level_video_length_fit <- run_brms_model(
  "exposure_level_video_length",
  bf(score | trials(trials) ~ condition*video_length + (1 | course_id/participant_id) + (0 + exposure + condition | course_id)),
  exposure_level_model_data,
  priors
)
```

```{r}
#| label: exposure-level video_length fixed effects
#| echo: false

fixef(exposure_level_video_length_fit)
```

### avg_time_of_prequestions: avg percent of the video elapsed when prequestions are addressed

```{r}
#| label: exposure-level time of prequestions
#| include: false

exposure_level_avg_time_of_prequestions_fit <- run_brms_model(
  "exposure_level_avg_time_of_prequestions",
  bf(score | trials(trials) ~ condition*avg_time_of_prequestions + (1 | course_id/participant_id) + (0 + exposure + condition | course_id)),
  exposure_level_model_data,
  priors
)

```

```{r}
#| label: exposure-level avg_time_of_prequestions fixed effects
#| echo: false

fixef(exposure_level_avg_time_of_prequestions_fit)
```

### time_answering_preqs: cumulative time spent addressing prequestions in video

```{r}
#| label: exposure-level time addressing prequestions
#| include: false

exposure_level_time_answering_preqs_fit <- run_brms_model(
  "exposure_level_time_answering_preqs",
  bf(score | trials(trials) ~ condition*time_answering_preqs + (1 | course_id/participant_id) + (0 + exposure + condition | course_id)),
  exposure_level_model_data,
  priors
)

```

```{r}
#| label: exposure-level time answering fixed effects
#| echo: false

fixef(exposure_level_time_answering_preqs_fit)
```

### answer_not_provided: was there at least one prequestion where the answer was never presented (learner needed to infer answer)

```{r}
#| label: exposure-level answer not provided
#| include: false

exposure_level_answer_not_provided_fit <- run_brms_model(
  "exposure_level_answer_not_provided",
  bf(score | trials(trials) ~ condition*answer_not_provided + (1 | course_id/participant_id) + (0 + exposure + condition | course_id)),
  exposure_level_model_data,
  priors
)
```

```{r}
#| label: exposure-level answer not provided fixed effects
#| echo: false

fixef(exposure_level_answer_not_provided_fit)
```

### require_memorization: Does any prequestion involve memorization of a word/term/phrase?

```{r}
#| label: exposure-level require memorization
#| include: false

exposure_level_require_memorization_fit <- run_brms_model(
  "exposure_level_require_memorization",
  bf(score | trials(trials) ~ condition*require_memorization + (1 | course_id/participant_id) + (0 + exposure + condition | course_id)),
  exposure_level_model_data,
  priors
)
```

```{r}
#| label: exposure-level require memorization fixed effects
#| echo: false

fixef(exposure_level_require_memorization_fit)
```

### preq_difficulty: percent correct (overall) on the prequestions associated with the video

```{r}
#| label: exposure-level prequestion difficulty
#| include: false

exposure_level_preq_difficulty_fit <- run_brms_model(
  "exposure_level_preq_difficulty",
  bf(score | trials(trials) ~ condition*preq_difficulty + (1 | course_id/participant_id) + (0 + exposure + condition | course_id)),
  exposure_level_model_data,
  priors
)
```

```{r}
#| label: exposure-level prequestion difficulty fixed effects
#| echo: false

fixef(exposure_level_preq_difficulty_fit)
```

##  Plot of moderator effects

```{r}
#| label: fig-moderator-effects
#| fig-cap: Estimates of the interaction between prequestion condition and each moderator. The circle is the mean of the posterior, and the lines show the 95% highest-density interval. Estimates are in logits.
#| echo: false

moderator_fits <- list(
  class_level_exposure_fit,
  class_level_number_assignments_fit,
  class_level_grade_level_fit,
  student_level_pretest_fit,
  student_level_major_fit,
  student_level_prequestion_score_fit,
  submission_level_percent_viewed_fit,
  submission_level_initiate_playback_fit,
  submission_level_n_events_fit,
  submission_level_time_fit,
  exposure_level_delay_fit,
  exposure_level_pct_correct_fit,
  exposure_level_video_length_fit,
  exposure_level_avg_time_of_prequestions_fit,
  exposure_level_time_answering_preqs_fit,
  exposure_level_answer_not_provided_fit,
  exposure_level_require_memorization_fit,
  exposure_level_preq_difficulty_fit
)

# extract the interaction term from each model, using the summary of the model

moderator_forest_plot_data <- lapply(moderator_fits, function(fit){
  fixef(fit) %>% 
    as_tibble(rownames="Effect") %>%
    filter(str_detect(Effect, ":")) %>%
    select(Effect, Estimate, Q2.5, Q97.5)
}) %>% 
  bind_rows(.id="model") %>%
  mutate(
    model = 1:n(),
    pretty_model_level = c("Class", "Class", "Class", "Class", "Student", "Student", "Student", "Submission", "Submission", "Submission", "Submission", "Assignment", "Assignment", "Assignment", "Assignment", "Assignment", "Assignment", "Assignment", "Assignment"),
    effect = c("exposure_order", "nbr_other_assessments", "level", "level", "pretest", "in_major", "prequestion_score", "viewpct", "initiate_playback", "nevents", "submDaysBeforeDueDate", "delay", "pct_correct", "video_length", "avg_time_of_prequestions", "time_answering_preqs", "answer_not_provided", "require_memorization", "preq_difficulty"),
    pretty_name = c("Exposure order", "Number of other assessments", "Grade level: High School", "Grade Level: Middle School", "Pretest score", "Course in major", "Prequestion score", "Percentage of video viewed", "Initiated video playback", "Number of click events", "Submission time", "Days between assignment and exam", "Avg percent correct for exam items", "Video length", "Avg time in video of prequestions", "Time in video answering prequestions", "Answer to prequestions not provided in video", "Requires memorization", "Prequestion difficulty"),
    continuous_measure = c(FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, FALSE, TRUE)
)

ggplot(moderator_forest_plot_data, aes(x=Estimate, y=pretty_name, xmin=Q2.5, xmax=Q97.5, shape=continuous_measure))+
  geom_pointrange()+
  scale_shape_manual(values=c(15,16), name="", labels=c("Categorical measure", "Scaled continuous measure"))+
  facet_grid(pretty_model_level~., scales="free_y", space="free")+
  theme_bw()+
  theme(panel.grid.major.y=element_blank())+
  labs(x="Effect estimate", y="Moderator")+
  theme(legend.position = "bottom")

```

# Secondary Analyses

## Percentage of video viewed

### With all students

```{r}
#| label: percentage of video viewed all students
#| include: false

percent_viewed_fit <- run_brms_model(
  "secondary_percent_viewed",
  bf(viewpct ~ condition + (1 | course_id/participant_id) + (0 + exposure_order + condition | course_id)),
  submission_level_model_data,
  priors,
  family=gaussian(link="identity")
)
```

```{r}
#| echo: false

fixef(percent_viewed_fit)
```

### With only students who initiated playback

```{r}
#| label: percentage of video viewed with only students who initiated playback
#| include: false

percent_viewed_initiated_fit <- run_brms_model(
  "secondary_percent_viewed_initiated",
  bf(viewpct ~ condition + (1 | course_id/participant_id) + (0 + exposure_order + condition | course_id)),
  submission_level_model_data %>%
    filter(initiate_playback == 1),
  priors,
  family=gaussian(link="identity")
)
```

```{r}
#| echo: false

fixef(percent_viewed_initiated_fit)
```

## Did the student start the video at all?

```{r}
#| label: percentage of video viewed
#| include: false

did_view_fit <- run_brms_model(
  "secondary_did_view",
  bf(initiate_playback ~ condition + (1 | course_id/participant_id) + (0 + exposure_order + condition | course_id)),
  submission_level_model_data,
  priors,
  family=bernoulli(link="logit")
)
```

```{r}
#| echo: false

fixef(did_view_fit)
```

### With moderators

```{r}
#| label: Add initiate_playback to student-level model data
#| include: false

secondary_model_data <- student_level_model_data %>%
  left_join(submission_level_model_data %>% group_by(participant_id)  %>% select(participant_id, condition, course_id, initiate_playback, submDaysBeforeDueDate) %>% ungroup(), by=join_by(participant_id, course_id, condition))
```

#### Pretest Score

```{r}
#| label: Pretest moderator for iniated viewing
#| include: false

did_view_pretest_fit <- run_brms_model(
  "secondary_did_view_pretest",
  bf(initiate_playback ~ condition*pretest + (1 | course_id/participant_id) + (0 + exposure_order + pretest*condition | course_id)),
  secondary_model_data,
  priors,
  family=bernoulli(link="logit")
)
```


```{r}
#| echo: false

fixef(did_view_pretest_fit)
```
#### Prequestion Score

`initiate_playback ~ condition*prequestion_score + (1 | course_id/participant_id) + (0 + exposure_order + prequestion_score*condition | course_id)`
```{r}
#| label: Prequestion score moderator for iniated viewing
#| include: false

did_view_prequestion_score_fit <- run_brms_model(
  "secondary_did_view_prequestion_score",
  bf(initiate_playback ~ condition*prequestion_score + (1 | course_id/participant_id) + (0 + exposure_order + prequestion_score*condition | course_id)),
  secondary_model_data,
  priors,
  family=bernoulli(link="logit")
)
```

```{r}
#| echo: false

fixef(did_view_prequestion_score_fit)
```

##### Alternate version, treating the interaction as only a fixed effect.

`initiate_playback ~ condition*prequestion_score + (1 | course_id/participant_id) + (0 + exposure_order + prequestion_score + condition | course_id)`

```{r}
#| label: Prequestion score moderator for iniated viewing v2
#| include: false

did_view_prequestion_score_v2_fit <- run_brms_model(
  "secondary_did_view_prequestion_score_v2",
  bf(initiate_playback ~ condition*prequestion_score + (1 | course_id/participant_id) + (0 + exposure_order + prequestion_score + condition | course_id)),
  secondary_model_data,
  priors,
  family=bernoulli(link="logit")
)
```

```{r}
#| echo: false

fixef(did_view_prequestion_score_v2_fit)
```

#### In Major

```{r}
#| label: In major moderator for iniated viewing
#| include: false

did_view_major_fit <- run_brms_model(
  "secondary_did_view_major",
  bf(initiate_playback ~ condition*in_major + (1 | course_id/participant_id) + (0 + exposure_order + in_major*condition | course_id)),
  secondary_model_data %>% mutate(initiate_playback = as.integer(initiate_playback)),
  priors,
  family=bernoulli(link="logit")
)
```

```{r}
#| echo: false

fixef(did_view_major_fit)
```

#### Grade Level

```{r}
#| label: Grade level moderator for iniated viewing
#| include: false

did_view_grade_fit <- run_brms_model(
  "secondary_did_view_grade",
  bf(initiate_playback ~ condition*level + (1 | course_id/participant_id) + (0 + exposure_order + level*condition | course_id)),
  secondary_model_data,
  priors,
  family=bernoulli(link="logit")
)
```

```{r}
#| echo: false

fixef(did_view_grade_fit)
```

#### Submission Time

```{r}
#| label: Submission time moderator for iniated viewing
#| include: false

did_view_time_fit <- run_brms_model(
  "secondary_did_view_time",
  bf(initiate_playback ~ condition*submDaysBeforeDueDate + (1 | course_id/participant_id) + (0 + exposure_order + submDaysBeforeDueDate*condition | course_id)),
  secondary_model_data,
  priors,
  family=bernoulli(link="logit")
)
```

```{r}
#| echo: false

fixef(did_view_time_fit)
```