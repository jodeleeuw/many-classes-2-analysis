---
title: "ManyClasses 2 Analysis"
format: html
editor: visual
---

```{r}
#| label: Load packages
#| include: false
library(osfr)
library(brms)
library(tidyr)
library(dplyr)
library(tidybayes)
library(ggplot2)
library(ggdist)
library(bayesplot)
library(forcats)
```

```{r}
#| label: Get data from OSF
#| include: false

osf_retrieve_node("https://osf.io/t5ma3/") %>%
  osf_ls_files(path="Analysis", pattern=".Rdata") %>%
  osf_download(path="data", conflicts="skip")
```

```{r}
#| label: Load data
#| include: false

load("data/analysis_set.Rdata")

```

# Raw data visualization

```{r}
#| label: Prepare data
#| include: false

overall_model_data <- datFrm %>%
  select(participant_id, course_id, outcome_prequestions, 
         points_possible_prequestions, outcome_control, 
         points_possible_control) %>%
  pivot_longer(cols = c(outcome_prequestions, outcome_control), 
               names_to = "outcome_phase", values_to = "score",
               names_prefix = "outcome_") %>%
  pivot_longer(cols = c(points_possible_prequestions, points_possible_control), 
    names_to = "trials_phase", 
    values_to = "trials", 
    names_prefix = "points_possible_") %>%
  filter(outcome_phase == trials_phase) %>%
  mutate(condition = outcome_phase) %>%
  select(-trials_phase, -outcome_phase)

```

```{r}
#| label: fig-raw-data
#| echo: false
#| fig-cap: The benefit of prequestions for each class, calculated as the proportion of correct responses in the prequestions condition minus the proportion of correct responses in the control condition. Dots are the mean and lines show +/- 1SE.

raw_data_summary <- overall_model_data %>%
  mutate(proportion_correct = score / trials) %>%
  pivot_wider(id_cols = c(participant_id, course_id), names_from=condition, values_from = proportion_correct) %>%
  mutate(prequestion_benefit = prequestions - control) %>%
  group_by(course_id) %>%
  mutate(course_id = factor(course_id)) %>%
  summarize(M = mean(prequestion_benefit), SD = sd(prequestion_benefit), SE = sd(prequestion_benefit) / sqrt(n()))

raw_data_summary$course_id <- fct_reorder(raw_data_summary$course_id, raw_data_summary$M, median)


ggplot(raw_data_summary, aes(x=M, y=course_id, xmin=M-SE, xmax=M+SE))+
  geom_pointrange()+
  theme_minimal()+
  theme(panel.grid.major.y=element_blank())+
  labs(x="Proportion correct with prequestions - proportion correct without",
       y="Class ID")
```

# Overall Model

```{r}
#| label: Helper function for running models
#| include: false

run_brms_model <- function(filename, formula, data, priors, bulk_ess_threshold=1000) {
  
  cores_to_use <- 6
  
  filepath <- paste0("fits/", filename, ".rds")
  
  if(file.exists(filepath)){
    fit <- readRDS(filepath)
  } else {
    fit <- brm(
      formula = formula,
      data = data,
      family = binomial(link = "logit"),
      chains = cores_to_use,
      iter = bulk_ess_threshold + 1000,
      warmup = 1000,
      cores = cores_to_use,
      control=list(
        adapt_delta=0.99
      ),
      prior=priors
    )
    
    ms <- summary(fit)
      
    ess <- c(
      sapply(ms$random, function(x){ return(x$Bulk_ESS)}, simplify = TRUE) %>% unlist(),
      ms$fixed$Bulk_ESS
    )
    
    min_ess <- min(ess)
    
    while(min_ess < bulk_ess_threshold){
      new_fit <- brm(
        formula = formula,
        data = data,
        family = binomial(link = "logit"),
        chains = cores_to_use,
        iter = bulk_ess_threshold + 1000,
        warmup = 1000,
        cores = cores_to_use,
        control=list(
          adapt_delta=0.99
        ),
        prior=priors
      )
      fit <- combine_models(fit, new_fit)
      
      ms <- summary(fit)
      
      ess <- c(
        sapply(ms$random, function(x){ return(x$Bulk_ESS)}, simplify = TRUE) %>% unlist(),
        ms$fixed$Bulk_ESS
      )
      
      min_ess <- min(ess)

    }
    saveRDS(fit, filepath)
  }
  return(fit)
}
```

This model is `score | trials(trials) ~ condition + (0 + condition | course_id) + (1 | course_id/participant_id)`. A fixed effect of condition (prequestions vs. control) and a random effect of condition per class, and random intercept of participant , nested in random intercept of class.

```{r}
#| label: Define priors
#| include: false

priors <- c(
  set_prior("normal(0, 0.5)", class="b"),
  set_prior("gamma(1.64, 0.32)", class="sd")
)

```

```{r}
#| label: My own model version
#| include: false

overall_fit <- run_brms_model(
  filename="overall", 
  formula=bf(score | trials(trials) ~ condition + (0 + condition | course_id) + (1 | course_id/participant_id)),
  data=overall_model_data,
  priors=priors
)
```

```{r}
#| label: fig-overall-forest-plot
#| echo: false
#| fig-cap: Model estimates of the benefit of prequestions for each class, shown as an odds ratio (probability of correct responses in prequestions / probability of correct responses in control). The circle is the median of the posterior, the thicker line is a 66% highest-density interval, and the thin line is a 95% highest-density interval. The diamond at the bottom shows the overall effect across classes.
#| fig-width: 10

forest_plot_data <- overall_fit %>%
  spread_draws(b_Intercept, b_conditionprequestions, r_course_id[id, condition]) %>%
  filter(condition %in% c("conditionprequestions", "Intercept")) %>%
  pivot_wider(names_from = condition, values_from = r_course_id) %>%
  mutate(id = factor(id), 
         course_effect = conditionprequestions + b_conditionprequestions,
         odds = exp(course_effect)
  )
forest_plot_data$id <- fct_reorder(forest_plot_data$id, forest_plot_data$odds, median)

overall_estimate_forest_plot <- forest_plot_data %>%
  filter(id==98) %>%
  mutate(odds = exp(b_conditionprequestions),
         id=0)
  
ggplot(forest_plot_data, aes(x = odds, y=id)) +
  stat_pointinterval()+
  stat_pointinterval(data=overall_estimate_forest_plot, shape=18, point_size=5)+
  labs(x="Odds ratio for correct response with prequestions / without prequestions ", y="Class ID")+
  scale_x_continuous(breaks=c(seq(0,1,0.1),seq(1,10,1)))+
  coord_trans(x = 'log10') +
  geom_vline(xintercept=1)+
  theme_minimal()+
  theme(panel.grid.major.y = element_blank(), panel.grid.minor.x = element_blank())
  
```

# Moderators

```{r}
#| label: Load moderators
#| include: false

load('moderators.Rdata')
```

## Class Level

### Exposure order

```{r}
#| label: Exposure order data
#| include: false

exposure_order_model_data <- datFrm %>%
  select(participant_id, course_id, outcome_prequestions, 
         points_possible_prequestions, outcome_control, 
         points_possible_control, exposure_order) %>%
  pivot_longer(cols = c(outcome_prequestions, outcome_control), 
               names_to = "outcome_phase", values_to = "score",
               names_prefix = "outcome_") %>%
  pivot_longer(cols = c(points_possible_prequestions, points_possible_control), 
    names_to = "trials_phase", 
    values_to = "trials", 
    names_prefix = "points_possible_") %>%
  filter(outcome_phase == trials_phase) %>%
  mutate(condition = outcome_phase) %>%
  select(-trials_phase, -outcome_phase)

```

```{r}
#| label: Exposure fixed effect
#| include: false

class_level_exposure_fit <- run_brms_model(
  "class_level_exposure",
  bf(score | trials(trials) ~ condition*exposure_order + (0 + condition + exposure_order | course_id) + (1 | course_id/participant_id)),
  exposure_order_model_data,
  priors
)
```

```{r}
#| echo: false
fixef(class_level_exposure_fit)
```

```{r}
#| label: Exposure order moderator
#| include: false

exposure_fit <- run_brms_model(
  "exposure_order",
  bf(score | trials(trials) ~ condition + (0 + condition + exposure_order | course_id) + (1 | course_id/participant_id)),
  exposure_order_model_data,
  priors
)

```

```{r}
#| label: fig-overall-forest-plot-with-exposure-order
#| echo: false
#| fig-cap: Model estimates of the benefit of prequestions for each class, taking into account assignment-level effects, shown as an odds ratio (probability of correct responses in prequestions / probability of correct responses in control). The circle is the median of the posterior, the thicker line is a 66% highest-density interval, and the thin line is a 95% highest-density interval. The diamond at the bottom shows the overall effect across classes.

forest_plot_data <- exposure_fit %>%
  spread_draws(b_Intercept, b_conditionprequestions, r_course_id[id, condition]) %>%
  filter(condition %in% c("conditionprequestions", "Intercept")) %>%
  pivot_wider(names_from = condition, values_from = r_course_id) %>%
  mutate(id = factor(id), 
         course_effect = conditionprequestions + b_conditionprequestions,
         odds = exp(course_effect)
  )
forest_plot_data$id <- fct_reorder(forest_plot_data$id, forest_plot_data$odds, median)

overall_estimate_forest_plot <- forest_plot_data %>%
  filter(id==98) %>%
  mutate(odds = exp(b_conditionprequestions),
         id=0)
  
ggplot(forest_plot_data, aes(x = odds, y=id)) +
  stat_pointinterval()+
  stat_pointinterval(data=overall_estimate_forest_plot, shape=18, point_size=5)+
  labs(x="Odds ratio for correct response with prequestions / without prequestions ", y="Class ID")+
  scale_x_continuous(breaks=c(seq(0,1,0.1),seq(1,10,1)))+
  coord_trans(x = 'log10') +
  geom_vline(xintercept=1)+
  theme_minimal()+
  theme(panel.grid.major.y = element_blank(), panel.grid.minor.x = element_blank())
```


```{r}
#| label: Merge class level moderators
#| include: false

class_level_model_data <- exposure_order_model_data %>%
  left_join(mod_class, by=join_by(course_id))
```

### Number of Assignments

```{r}
#| label: Class-level number of assignments
#| include: false

class_level_number_assignments_fit <- run_brms_model(
  "class_level_number_assignments",
  bf(score | trials(trials) ~ condition*nbr_other_assessments + (1 | course_id/participant_id) + (0 + condition + exposure_order | course_id)),
  class_level_model_data,
  priors
)
```

```{r}
#| echo: false
fixef(class_level_number_assignments_fit)
```

## Student Level

```{r}
#| label: Create student-level model data
#| include: false

student_level_model_data <- exposure_order_model_data %>%
  left_join(mod_student, by=join_by(participant_id, course_id)) %>%
  mutate(pretest = pretest / 100) %>%
  mutate(level = case_when(
    level %in% c(6,7,8) ~ "middle school",
    level %in% c(9,10,11,12) ~ "high school",
    TRUE ~ "college"
  ))
```

### Grade Level

```{r}
#| label: student-level grade level moderator
#| include: false

student_level_grade_level_fit <- run_brms_model(
  "student_level_grade_level",
  bf(score | trials(trials) ~ condition*level + (1 | course_id/participant_id) + (0 + exposure_order + condition | course_id)),
  student_level_model_data,
  priors
)
```

```{r}
#| echo: false
fixef(student_level_grade_level_fit)
```

### Pretest

*Note*: We get a warning that NAs are excluded.

```{r}
#| label: student-level pretest moderator
#| include: false

student_level_pretest_fit <- run_brms_model(
  "student_level_pretest",
  bf(score | trials(trials) ~ condition*pretest + (1 | course_id/participant_id) + (0 + exposure_order + condition*pretest | course_id)),
  student_level_model_data,
  priors
)
```

```{r}
#| echo: false
fixef(student_level_pretest_fit)
```

### In Major

```{r}
#| label: student-level in major moderator
#| include: false

student_level_major_fit <- run_brms_model(
  "student_level_major",
  bf(score | trials(trials) ~ condition*in_major + (1 | course_id/participant_id) + (0 + exposure_order + in_major*condition | course_id)),
  student_level_model_data,
  priors
)
```

```{r}
#| echo: false
fixef(student_level_major_fit)
```

### Prequestion Score

```{r}
#| label: student-level prequestion score moderator
#| include: false

student_level_prequestion_score_fit <- run_brms_model(
  "student_level_prequestion_score",
  bf(score | trials(trials) ~ condition*prequestion_score + (1 | course_id/participant_id) + (0 + exposure_order + prequestion_score*condition | course_id)),
  student_level_model_data,
  priors
)

```

```{r}
#| echo: false
fixef(student_level_prequestion_score_fit)
```

## Submission Level

```{r}
#| label: Create submission-level data
#| include: false

submission_level_model_data <- exposure_order_model_data %>%
  left_join(mod_submission, by=join_by(participant_id, course_id, condition))

```

### Percentage of video viewed

```{r}
#| label: submission-level percentage of video video
#| include: false

submission_level_percent_viewed_fit <- run_brms_model(
  "submission_level_percent_viewed",
  bf(score | trials(trials) ~ condition*viewpct + (1 | course_id/participant_id) + (0 + exposure_order + viewpct*condition | course_id)),
  submission_level_model_data,
  priors
)

```

```{r}
#| echo: false
fixef(submission_level_percent_viewed_fit)
```

### Did the student start the video at all?

```{r}
#| label: submission-level initiate playback
#| include: false

submission_level_initiate_playback_fit <- run_brms_model(
  "submission_level_initiate_playback",
  bf(score | trials(trials) ~ condition*initiate_playback + (1 | course_id/participant_id) + (0 + exposure_order + initiate_playback*condition | course_id)),
  submission_level_model_data,
  priors
)

```

```{r}
#| echo: false
fixef(submission_level_initiate_playback_fit)
```

### N Events (clicks?)

```{r}
#| label: submission-level number of events
#| include: false

submission_level_n_events_fit <- run_brms_model(
  "submission_level_n_events",
  bf(score | trials(trials) ~ condition*nevents + (1 | course_id/participant_id) + (0 + exposure_order + nevents*condition | course_id)),
  submission_level_model_data,
  priors
)

```

```{r}
#| echo: false
fixef(submission_level_n_events_fit)
```

### Duration

```{r}
#| label: submission-level duration
#| include: false
#| eval: false

submission_level_duration_fit <- run_brms_model(
  "submission_level_duration",
  bf(score | trials(trials) ~ condition*duration + (1 | course_id/participant_id) + (0 + exposure_order + duration*condition | course_id)),
  submission_level_model_data,
  priors
)

```

This model won't run.

```{r}
#| echo: false
#| eval: false
fixef(submission_level_duration_fit)
```

### Submission time

```{r}
#| label: submission-level time
#| include: false

submission_level_time_fit <- run_brms_model(
  "submission_level_time",
  bf(score | trials(trials) ~ condition*submDaysBeforeDueDate + (1 | course_id/participant_id) + (0 + exposure_order + submDaysBeforeDueDate*condition | course_id)),
  submission_level_model_data,
  priors
)

```

```{r}
#| echo: false
fixef(submission_level_time_fit)
```

## Exposure Level

```{r}
#| label: Create exposure-level data
#| include: false

exposure_level_model_data <- exposure_order_model_data %>%
  mutate(exposure = case_when(
    exposure_order == 'control_then_prequestion' & condition == 'control' ~ 1,
    exposure_order == 'control_then_prequestion' & condition == 'prequestions' ~ 2,
    exposure_order == 'prequestions_then_control' & condition == 'control' ~ 2,
    exposure_order == 'prequestions_then_control' & condition == 'prequestions' ~ 1,
  )) %>%
  left_join(mod_exposure, by=join_by(course_id, exposure)) 
```

Current thinking about model structure: we don't have enough different exposures in each class to get class-level estimates of the exposure-level moderators. I'm including intercepts for courses and participants, as well as exposure-level slopes are condition effects within classes. The moderator is then estimated only at the population level. Theoretically I think we'd expect these to vary across classes or even across individuals, but I don't think we have the data to fit that kind of model.

### delay: days between assignment due date and exam date

```{r}
#| label: exposure-level delay
#| include: false

exposure_level_delay_fit <- run_brms_model(
  "exposure_level_delay",
  bf(score | trials(trials) ~ condition*delay + (1 | course_id/participant_id) + (0 + exposure + condition | course_id)),
  exposure_level_model_data,
  priors
)
```

```{r}
#| echo: false
fixef(exposure_level_delay_fit)
```

### pct_correct: overall percent correct on the exam items

```{r}
#| label: exposure-level percent correct
#| include: false

exposure_level_pct_correct_fit <- run_brms_model(
  "exposure_level_pct_correct",
  bf(score | trials(trials) ~ condition*pct_correct + (1 | course_id/participant_id) + (0 + exposure + condition | course_id)),
  exposure_level_model_data,
  priors
)

```

```{r}
#| label: exposure-level pct_correct fixed effects
#| echo: false

fixef(exposure_level_pct_correct_fit)
```

### video_length: total duration of video

```{r}
#| label: exposure-level video length
#| include: false

exposure_level_video_length_fit <- run_brms_model(
  "exposure_level_video_length",
  bf(score | trials(trials) ~ condition*video_length + (1 | course_id/participant_id) + (0 + exposure + condition | course_id)),
  exposure_level_model_data,
  priors
)
```

```{r}
#| label: exposure-level video_length fixed effects
#| echo: false

fixef(exposure_level_video_length_fit)
```

### avg_time_of_prequestions: avg percent of the video elapsed when prequestions are addressed

```{r}
#| label: exposure-level time of prequestions
#| include: false

exposure_level_avg_time_of_prequestions_fit <- run_brms_model(
  "exposure_level_avg_time_of_prequestions",
  bf(score | trials(trials) ~ condition*avg_time_of_prequestions + (1 | course_id/participant_id) + (0 + exposure + condition | course_id)),
  exposure_level_model_data,
  priors
)

```

```{r}
#| label: exposure-level avg_time_of_prequestions fixed effects
#| echo: false

fixef(exposure_level_avg_time_of_prequestions_fit)
```

### time_answering_preqs: cumulative time spent addressing prequestions in video

```{r}
#| label: exposure-level time addressing prequestions
#| include: false

exposure_level_time_answering_preqs_fit <- run_brms_model(
  "exposure_level_time_answering_preqs",
  bf(score | trials(trials) ~ condition*time_answering_preqs + (1 | course_id/participant_id) + (0 + exposure + condition | course_id)),
  exposure_level_model_data,
  priors
)

```

```{r}
#| label: exposure-level time answering fixed effects
#| echo: false

fixef(exposure_level_time_answering_preqs_fit)
```

### answer_not_provided: was there at least one prequestion where the answer was never presented (learner needed to infer answer)

```{r}
#| label: exposure-level answer not provided
#| include: false

exposure_level_answer_not_provided_fit <- run_brms_model(
  "exposure_level_answer_not_provided",
  bf(score | trials(trials) ~ condition*answer_not_provided + (1 | course_id/participant_id) + (0 + exposure + condition | course_id)),
  exposure_level_model_data,
  priors
)
```

```{r}
#| label: exposure-level answer not provided fixed effects
#| echo: false

fixef(exposure_level_answer_not_provided_fit)
```

### require_memorization: Does any prequestion involve memorization of a word/term/phrase?

```{r}
#| label: exposure-level require memorization
#| include: false

exposure_level_require_memorization_fit <- run_brms_model(
  "exposure_level_require_memorization",
  bf(score | trials(trials) ~ condition*require_memorization + (1 | course_id/participant_id) + (0 + exposure + condition | course_id)),
  exposure_level_model_data,
  priors
)
```

```{r}
#| label: exposure-level require memorization fixed effects
#| echo: false

fixef(exposure_level_require_memorization_fit)
```

### preq_difficulty: percent correct (overall) on the prequestions associated with the video

```{r}
#| label: exposure-level prequestion difficulty
#| include: false

exposure_level_preq_difficulty_fit <- run_brms_model(
  "exposure_level_preq_difficulty",
  bf(score | trials(trials) ~ condition*preq_difficulty + (1 | course_id/participant_id) + (0 + exposure + condition | course_id)),
  exposure_level_model_data,
  priors
)
```

```{r}
#| label: exposure-level prequestion difficulty fixed effects
#| echo: false

fixef(exposure_level_preq_difficulty_fit)
```

# TODO list

- [x] Figure 1: Fix spelling of "proption"
- [x] Figure 3: "pre-questions" -> "prequestions" (for consistency)
- [ ] On one or both figures, add to y-axis in addition to class id: N, pct correct prequestions, pct correct control, {any others}
- [ ] Make pretty for publication
- [x] Run a model where exposure order is a fixed effect â€“ we anticipate that there will be no effect, and that this will provide support for our decision to include exposure order as a class-level random effect
- [x] Change the "Level" outcome to a categorical variable:
    level %in% c(6,7,8) ~ "middle school"
    level %in% c(9,10,11,12) ~ "high school"
    (else) true ~ "college"
- [x] Rerun grade level model
- [x] Use the priors from https://osf.io/5atq3
- [x] Can you please add the component to the project site at: https://osf.io/t5ma3/? -- I don't want to accidentally set things up wrong.
- [ ] Upload model fits to OSF
- [ ] Simple models where "Percentage of video viewed" and "Did the student start the video at all?" are the outcomes (DVs), with condition as the IV and no interaction terms.
- [ ] Limit to only those students who did initiate playback, and re-run the simple model above, where "Percentage of video viewed" is the DV and condition is the IV.
- [ ] Models where "Did the student start the video at all?" is the DV, and where we include condition X moderator interactions.  For these, it doesn't really make sense to include any exposure-level interaction terms regarding the video (e.g., duration, was the question answered) because students didn't watch at least one video, but minimally we should measure the interaction with:
    - [ ] Pretest
    - [ ] Prequestion score
    - [ ] In Major
    - [ ] Grade level
    - [ ] Submission time