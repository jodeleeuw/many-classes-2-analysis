---
title: "ManyClasses 2 Analysis"
format: html
editor: visual
---

```{r}
#| label: Load packages
#| include: false
library(osfr)
library(brms)
library(tidyr)
library(dplyr)
library(tidybayes)
library(ggplot2)
library(ggdist)
library(bayesplot)
library(forcats)
```

```{r}
#| label: Get data from OSF
#| include: false

osf_retrieve_node("https://osf.io/t5ma3/") %>%
  osf_ls_files(path="Analysis", pattern=".Rdata") %>%
  osf_download(path="data", conflicts="skip")
```

```{r}
#| label: Load data
#| include: false

load("data/analysis_set.Rdata")

```

# Raw data visualization

```{r}
#| label: Prepare data
#| include: false

overall_model_data <- datFrm %>%
  select(participant_id, course_id, outcome_prequestions, 
         points_possible_prequestions, outcome_control, 
         points_possible_control) %>%
  pivot_longer(cols = c(outcome_prequestions, outcome_control), 
               names_to = "outcome_phase", values_to = "score",
               names_prefix = "outcome_") %>%
  pivot_longer(cols = c(points_possible_prequestions, points_possible_control), 
    names_to = "trials_phase", 
    values_to = "trials", 
    names_prefix = "points_possible_") %>%
  filter(outcome_phase == trials_phase) %>%
  mutate(condition = outcome_phase) %>%
  select(-trials_phase, -outcome_phase)

```

```{r}
#| label: fig-raw-data
#| echo: false
#| fig-cap: The benefit of prequestions for each class, calculated as the proportion of correct responses in the prequestions condition minus the proportion of correct responses in the control condition. Dots are the mean and lines show +/- 1SE.

raw_data_summary <- overall_model_data %>%
  mutate(proportion_correct = score / trials) %>%
  pivot_wider(id_cols = c(participant_id, course_id), names_from=condition, values_from = proportion_correct) %>%
  mutate(prequestion_benefit = prequestions - control) %>%
  group_by(course_id) %>%
  mutate(course_id = factor(course_id)) %>%
  summarize(M = mean(prequestion_benefit), SD = sd(prequestion_benefit), SE = sd(prequestion_benefit) / sqrt(n()))

raw_data_summary$course_id <- fct_reorder(raw_data_summary$course_id, raw_data_summary$M, median)


ggplot(raw_data_summary, aes(x=M, y=course_id, xmin=M-SE, xmax=M+SE))+
  geom_pointrange()+
  theme_minimal()+
  theme(panel.grid.major.y=element_blank())+
  labs(x="Proption correct with prequestions - proportion correct without",
       y="Class ID")
```

# Overall Model

```{r}
#| label: Helper function for running models
#| include: false

run_brms_model <- function(filename, formula, data, priors, bulk_ess_threshold=1000) {
  
  cores_to_use <- 6
  
  filepath <- paste0("fits/", filename, ".rds")
  
  if(file.exists(filepath)){
    fit <- readRDS(filepath)
  } else {
    fit <- brm(
      formula = formula,
      data = data,
      family = binomial(link = "logit"),
      chains = cores_to_use,
      iter = bulk_ess_threshold + 1000,
      warmup = 1000,
      cores = cores_to_use,
      control=list(
        adapt_delta=0.99
      ),
      prior=priors
    )
    
    ms <- summary(fit)
      
    ess <- c(
      sapply(ms$random, function(x){ return(x$Bulk_ESS)}, simplify = TRUE) %>% unlist(),
      ms$fixed$Bulk_ESS
    )
    
    min_ess <- min(ess)
    
    while(min_ess < bulk_ess_threshold){
      new_fit <- brm(
        formula = formula,
        data = data,
        family = binomial(link = "logit"),
        chains = cores_to_use,
        iter = bulk_ess_threshold + 1000,
        warmup = 1000,
        cores = cores_to_use,
        control=list(
          adapt_delta=0.99
        ),
        prior=priors
      )
      fit <- combine_models(fit, new_fit)
      
      ms <- summary(fit)
      
      ess <- c(
        sapply(ms$random, function(x){ return(x$Bulk_ESS)}, simplify = TRUE) %>% unlist(),
        ms$fixed$Bulk_ESS
      )
      
      min_ess <- min(ess)

    }
    saveRDS(fit, filepath)
  }
  return(fit)
}
```

This model is `score | trials(trials) ~ condition + (0 + condition | course_id) + (1 | course_id/participant_id)`. A fixed effect of condition (prequestions vs. control) and a random effect of condition per class, and random intercept of participant , nested in random intercept of class.

```{r}
#| label: Define priors
#| include: false

priors <- c(
  set_prior("normal(0, 0.5)", class="b"),
  set_prior("gamma(1.64, 0.32)", class="sd")
)

```

```{r}
#| label: My own model version
#| include: false

overall_fit <- run_brms_model(
  filename="overall", 
  formula=bf(score | trials(trials) ~ condition + (0 + condition | course_id) + (1 | course_id/participant_id)),
  data=overall_model_data,
  priors=priors
)
```

```{r}
#| label: fig-overall-forest-plot
#| echo: false
#| fig-cap: Model estimates of the benefit of prequestions for each class, shown as an odds ratio (probability of correct responses in prequestions / probability of correct responses in control). The circle is the median of the posterior, the thicker line is a 66% highest-density interval, and the thin line is a 95% highest-density interval. The diamond at the bottom shows the overall effect across classes.
#| fig-width: 10

forest_plot_data <- overall_fit %>%
  spread_draws(b_Intercept, b_conditionprequestions, r_course_id[id, condition]) %>%
  filter(condition %in% c("conditionprequestions", "Intercept")) %>%
  pivot_wider(names_from = condition, values_from = r_course_id) %>%
  mutate(id = factor(id), 
         course_effect = conditionprequestions + b_conditionprequestions,
         odds = exp(course_effect)
  )
forest_plot_data$id <- fct_reorder(forest_plot_data$id, forest_plot_data$odds, median)

overall_estimate_forest_plot <- forest_plot_data %>%
  filter(id==98) %>%
  mutate(odds = exp(b_conditionprequestions),
         id=0)
  
ggplot(forest_plot_data, aes(x = odds, y=id)) +
  stat_pointinterval()+
  stat_pointinterval(data=overall_estimate_forest_plot, shape=18, point_size=5)+
  labs(x="Odds ratio for correct response with pre-questions / without pre-questions ", y="Class ID")+
  scale_x_continuous(breaks=c(seq(0,1,0.1),seq(1,10,1)))+
  coord_trans(x = 'log10') +
  geom_vline(xintercept=1)+
  theme_minimal()+
  theme(panel.grid.major.y = element_blank(), panel.grid.minor.x = element_blank())
  
```

# Moderators

```{r}
#| label: Load moderators
#| include: false

load('moderators.Rdata')
```

## Exposure order

Note taking for myself: the idea here is that there are different assignments in each class, and exposure order is basically which assignment people got. So wouldn't we expect 0 fixed effect, since there's no reason to expect the first assignments to be systematically different from the second assignments across classes? Should this be a random class-level effect?

```{r}
#| label: Exposure order data
#| include: false

exposure_order_model_data <- datFrm %>%
  select(participant_id, course_id, outcome_prequestions, 
         points_possible_prequestions, outcome_control, 
         points_possible_control, exposure_order) %>%
  pivot_longer(cols = c(outcome_prequestions, outcome_control), 
               names_to = "outcome_phase", values_to = "score",
               names_prefix = "outcome_") %>%
  pivot_longer(cols = c(points_possible_prequestions, points_possible_control), 
    names_to = "trials_phase", 
    values_to = "trials", 
    names_prefix = "points_possible_") %>%
  filter(outcome_phase == trials_phase) %>%
  mutate(condition = outcome_phase) %>%
  select(-trials_phase, -outcome_phase)

```

```{r}
#| label: Exposure order moderator
#| include: false

exposure_fit <- run_brms_model(
  "exposure_order",
  bf(score | trials(trials) ~ condition + (0 + condition + exposure_order | course_id) + (1 | course_id/participant_id)),
  exposure_order_model_data,
  priors
)

```

```{r}
#| label: fig-overall-forest-plot-with-exposure-order
#| echo: false
#| fig-cap: Model estimates of the benefit of prequestions for each class, taking into account assignment-level effects, shown as an odds ratio (probability of correct responses in prequestions / probability of correct responses in control). The circle is the median of the posterior, the thicker line is a 66% highest-density interval, and the thin line is a 95% highest-density interval. The diamond at the bottom shows the overall effect across classes.

forest_plot_data <- exposure_fit %>%
  spread_draws(b_Intercept, b_conditionprequestions, r_course_id[id, condition]) %>%
  filter(condition %in% c("conditionprequestions", "Intercept")) %>%
  pivot_wider(names_from = condition, values_from = r_course_id) %>%
  mutate(id = factor(id), 
         course_effect = conditionprequestions + b_conditionprequestions,
         odds = exp(course_effect)
  )
forest_plot_data$id <- fct_reorder(forest_plot_data$id, forest_plot_data$odds, median)

overall_estimate_forest_plot <- forest_plot_data %>%
  filter(id==98) %>%
  mutate(odds = exp(b_conditionprequestions),
         id=0)
  
ggplot(forest_plot_data, aes(x = odds, y=id)) +
  stat_pointinterval()+
  stat_pointinterval(data=overall_estimate_forest_plot, shape=18, point_size=5)+
  labs(x="Odds ratio for correct response with pre-questions / without pre-questions ", y="Class ID")+
  scale_x_continuous(breaks=c(seq(0,1,0.1),seq(1,10,1)))+
  coord_trans(x = 'log10') +
  geom_vline(xintercept=1)+
  theme_minimal()+
  theme(panel.grid.major.y = element_blank(), panel.grid.minor.x = element_blank())
```

## Class Level

```{r}
#| label: Merge class level moderators
#| include: false

class_level_model_data <- exposure_order_model_data %>%
  left_join(mod_class, by=join_by(course_id))
```

### Number of Assignments

```{r}
#| label: Class-level number of assignments
#| include: false

class_level_number_assignments_fit <- run_brms_model(
  "class_level_number_assignments",
  bf(score | trials(trials) ~ condition*nbr_other_assessments + (1 | course_id/participant_id) + (0 + condition + exposure_order | course_id)),
  class_level_model_data,
  priors
)
```

```{r}
#| echo: false
fixef(class_level_number_assignments_fit)
```

## Student Level

```{r}
#| label: Create student-level model data
#| include: false

student_level_model_data <- exposure_order_model_data %>%
  left_join(mod_student, by=join_by(participant_id, course_id)) %>%
  mutate(pretest = pretest / 100)

```

### Grade Level

Question about this one: we can't nest this in classes, right? Because there are classes with all one grade level? And we're more interested in how this behaves at the class level?

```{r}
#| label: student-level grade level moderator
#| include: false

student_level_grade_level_fit <- run_brms_model(
  "student_level_grade_level",
  bf(score | trials(trials) ~ condition*level + (1 | course_id/participant_id) + (0 + exposure_order + condition | course_id)),
  student_level_model_data,
  priors
)
```

```{r}
#| echo: false
fixef(student_level_grade_level_fit)
```

### Pretest

*Note*: We get a warning that NAs are excluded.

```{r}
#| label: student-level pretest moderator
#| include: false

student_level_pretest_fit <- run_brms_model(
  "student_level_pretest",
  bf(score | trials(trials) ~ condition*pretest + (1 | course_id/participant_id) + (0 + exposure_order + condition*pretest | course_id)),
  student_level_model_data,
  priors
)
```

```{r}
#| echo: false
fixef(student_level_pretest_fit)
```

### In Major

```{r}
#| label: student-level in major moderator
#| include: false

student_level_major_fit <- run_brms_model(
  "student_level_major",
  bf(score | trials(trials) ~ condition*in_major + (1 | course_id/participant_id) + (0 + exposure_order + in_major*condition | course_id)),
  student_level_model_data,
  priors
)
```

```{r}
#| echo: false
fixef(student_level_major_fit)
```

### Prequestion Score

*Question: what does it mean to get prequestion score in control condition*

```{r}
#| label: student-level prequestion score moderator
#| include: false

student_level_prequestion_score_fit <- run_brms_model(
  "student_level_prequestion_score",
  bf(score | trials(trials) ~ condition*prequestion_score + (1 | course_id/participant_id) + (0 + exposure_order + prequestion_score*condition | course_id)),
  student_level_model_data,
  priors
)

```

```{r}
#| echo: false
fixef(student_level_prequestion_score_fit)
```

## Submission Level

```{r}
#| label: Create submission-level data
#| include: false

submission_level_model_data <- exposure_order_model_data %>%
  left_join(mod_submission, by=join_by(participant_id, course_id, condition))

```

### Percentage of video viewed

```{r}
#| label: submission-level percentage of video video
#| include: false

submission_level_percent_viewed_fit <- run_brms_model(
  "submission_level_percent_viewed",
  bf(score | trials(trials) ~ condition*viewpct + (1 | course_id/participant_id) + (0 + exposure_order + viewpct*condition | course_id)),
  submission_level_model_data,
  priors
)

```

```{r}
#| echo: false
fixef(submission_level_percent_viewed_fit)
```

### Did the student start the video at all?

```{r}
#| label: submission-level initiate playback
#| include: false

submission_level_initiate_playback_fit <- run_brms_model(
  "submission_level_initiate_playback",
  bf(score | trials(trials) ~ condition*initiate_playback + (1 | course_id/participant_id) + (0 + exposure_order + initiate_playback*condition | course_id)),
  submission_level_model_data,
  priors
)

```

```{r}
#| echo: false
fixef(submission_level_initiate_playback_fit)
```

### N Events (clicks?)

```{r}
#| label: submission-level number of events
#| include: false

submission_level_n_events_fit <- run_brms_model(
  "submission_level_n_events",
  bf(score | trials(trials) ~ condition*nevents + (1 | course_id/participant_id) + (0 + exposure_order + nevents*condition | course_id)),
  submission_level_model_data,
  priors
)

```

```{r}
#| echo: false
fixef(submission_level_n_events_fit)
```

### Duration

```{r}
#| label: submission-level duration
#| include: false
#| eval: false

submission_level_duration_fit <- run_brms_model(
  "submission_level_duration",
  bf(score | trials(trials) ~ condition*duration + (1 | course_id/participant_id) + (0 + exposure_order + duration*condition | course_id)),
  submission_level_model_data,
  priors
)

```

This model won't run.

```{r}
#| echo: false
#| eval: false
fixef(submission_level_duration_fit)
```

### Submission time

```{r}
#| label: submission-level time
#| include: false

submission_level_time_fit <- run_brms_model(
  "submission_level_time",
  bf(score | trials(trials) ~ condition*submDaysBeforeDueDate + (1 | course_id/participant_id) + (0 + exposure_order + submDaysBeforeDueDate*condition | course_id)),
  submission_level_model_data,
  priors
)

```

```{r}
#| echo: false
fixef(submission_level_time_fit)
```

## Exposure Level

```{r}
#| label: Create exposure-level data
#| include: false

exposure_level_model_data <- exposure_order_model_data %>%
  mutate(exposure = case_when(
    exposure_order == 'control_then_prequestion' & condition == 'control' ~ 1,
    exposure_order == 'control_then_prequestion' & condition == 'prequestions' ~ 2,
    exposure_order == 'prequestions_then_control' & condition == 'control' ~ 2,
    exposure_order == 'prequestions_then_control' & condition == 'prequestions' ~ 1,
  )) %>%
  left_join(mod_exposure, by=join_by(course_id, exposure)) 
```

Current thinking about model structure: we don't have enough different exposures in each class to get class-level estimates of the exposure-level moderators. I'm including intercepts for courses and participants, as well as exposure-level slopes are condition effects within classes. The moderator is then estimated only at the population level. Theoretically I think we'd expect these to vary across classes or even across individuals, but I don't think we have the data to fit that kind of model.

### delay: days between assignment due date and exam date

```{r}
#| label: exposure-level delay
#| include: false

exposure_level_delay_fit <- run_brms_model(
  "exposure_level_delay",
  bf(score | trials(trials) ~ condition*delay + (1 | course_id/participant_id) + (0 + exposure + condition | course_id)),
  exposure_level_model_data,
  priors
)
```

```{r}
#| echo: false
fixef(exposure_level_delay_fit)
```

### pct_correct: overall percent correct on the exam items

```{r}
#| label: exposure-level percent correct
#| include: false

exposure_level_pct_correct_fit <- run_brms_model(
  "exposure_level_pct_correct",
  bf(score | trials(trials) ~ condition*pct_correct + (1 | course_id/participant_id) + (0 + exposure + condition | course_id)),
  exposure_level_model_data,
  priors
)

```

```{r}
#| label: exposure-level pct_correct fixed effects
#| echo: false

fixef(exposure_level_pct_correct_fit)
```

### video_length: total duration of video

```{r}
#| label: exposure-level video length
#| include: false

exposure_level_video_length_fit <- run_brms_model(
  "exposure_level_video_length",
  bf(score | trials(trials) ~ condition*video_length + (1 | course_id/participant_id) + (0 + exposure + condition | course_id)),
  exposure_level_model_data,
  priors
)
```

```{r}
#| label: exposure-level video_length fixed effects
#| echo: false

fixef(exposure_level_video_length_fit)
```

### avg_time_of_prequestions: avg percent of the video elapsed when prequestions are addressed

```{r}
#| label: exposure-level time of prequestions
#| include: false

exposure_level_avg_time_of_prequestions_fit <- run_brms_model(
  "exposure_level_avg_time_of_prequestions",
  bf(score | trials(trials) ~ condition*avg_time_of_prequestions + (1 | course_id/participant_id) + (0 + exposure + condition | course_id)),
  exposure_level_model_data,
  priors
)

```

```{r}
#| label: exposure-level avg_time_of_prequestions fixed effects
#| echo: false

fixef(exposure_level_avg_time_of_prequestions_fit)
```

### time_answering_preqs: cumulative time spent addressing prequestions in video

```{r}
#| label: exposure-level time addressing prequestions
#| include: false

exposure_level_time_answering_preqs_fit <- run_brms_model(
  "exposure_level_time_answering_preqs",
  bf(score | trials(trials) ~ condition*time_answering_preqs + (1 | course_id/participant_id) + (0 + exposure + condition | course_id)),
  exposure_level_model_data,
  priors
)

```

```{r}
#| label: exposure-level time answering fixed effects
#| echo: false

fixef(exposure_level_time_answering_preqs_fit)
```

### answer_not_provided: was there at least one prequestion where the answer was never presented (learner needed to infer answer)

```{r}
#| label: exposure-level answer not provided
#| include: false

exposure_level_answer_not_provided_fit <- run_brms_model(
  "exposure_level_answer_not_provided",
  bf(score | trials(trials) ~ condition*answer_not_provided + (1 | course_id/participant_id) + (0 + exposure + condition | course_id)),
  exposure_level_model_data,
  priors
)
```

```{r}
#| label: exposure-level answer not provided fixed effects
#| echo: false

fixef(exposure_level_answer_not_provided_fit)
```

### require_memorization: Does any prequestion involve memorization of a word/term/phrase?

```{r}
#| label: exposure-level require memorization
#| include: false

exposure_level_require_memorization_fit <- run_brms_model(
  "exposure_level_require_memorization",
  bf(score | trials(trials) ~ condition*require_memorization + (1 | course_id/participant_id) + (0 + exposure + condition | course_id)),
  exposure_level_model_data,
  priors
)
```

```{r}
#| label: exposure-level require memorization fixed effects
#| echo: false

fixef(exposure_level_require_memorization_fit)
```

### preq_difficulty: percent correct (overall) on the prequestions associated with the video

```{r}
#| label: exposure-level prequestion difficulty
#| include: false

exposure_level_preq_difficulty_fit <- run_brms_model(
  "exposure_level_preq_difficulty",
  bf(score | trials(trials) ~ condition*preq_difficulty + (1 | course_id/participant_id) + (0 + exposure + condition | course_id)),
  exposure_level_model_data,
  priors
)
```

```{r}
#| label: exposure-level prequestion difficulty fixed effects
#| echo: false

fixef(exposure_level_preq_difficulty_fit)
```

# notes / misc

## ben questions

-   priors for model?

-   exposure order moderator

-   moderators as main effects or interactions or both?

Fit class-level moderators using the model above, replacing exposure_order with class-level moderator.

Use a different model for participant-level moderators.

For assignment-level moderators, use assignment ID not nested in classes? Because there are only two assignments per class, nesting seems unhelpful. Instead allow for class-level intercept?

Also fit a model where the outcome is whether/how much of the video they watch. Whether = \> 5% of the video. Prediction is that prequestions will affect whether students watch the video.

------------------------------------------------------------------------

I also wrote a script to organize everything.  As you know, I've felt uncomfortable with the piss poor documentation I provided about the ManyClasses2 moderators.  So here (attached, and also in the ManyClasses 2 folder in Dropbox), you'll find a script that grabs everything from OSF and organizes it into four different data frames:

-   **mod_class**: Moderators at the class-level (30 obs) - key is course_id

-   **mod_exposure**: Moderators at the exposure-level (60 obs; exposures are analogous to but distinct from "periods" in a repeated-measures design; in any case, here there are two "exposures" per class in this study) - keys are course_id + exposure

-   **mod_student**: Moderators at the student-level (1571 obs) - key is participant_id

-   **mod_submission**: Moderators at the level of each student's interactions with a treatment (3142 obs; two observations per student, one for each exposure)

I hope this makes moderator analyses super-easy! 

Note: Right now we only have one moderator at the class-level (the number of other graded assessments in the course).  Obviously we could also do analyses on consent rate, enrollment, semester, institution, etc...  But these are atheoretical, and we already understand class-level moderators to be underpowered, so I've not added them.  If you think I should add 'em to mod_class, just let me know.

Appreciatively,

Ben
